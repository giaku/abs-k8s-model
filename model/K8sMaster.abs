module K8sMaster;

export *;

import * from ABS.DC;
import * from K8sService;
import * from K8sUtil;
import * from ABS.Scheduler;

data NodeType = NodeType(String typeName, Map<String,Int> podsNumber)

//
// MASTER
//
interface Master{
  Bool deployService(Service s);

  List<NodeState> getNodesStates();

  List<Pair<String,ConsumptionMap>> getNodesMap();

  Bool createNodes(Int nNodes, Rat cpu, Rat memory, Rat baseSystemLoad, Map<String,Map<Int,Map<String,Rat>>> calibrationMaps);

  Unit setSchedulerRulesMap(Map<String,List<Int>> rulesMap);

  MasterLoadbalancer getEndpoint();
}
class MasterObject(CloudProvider cp, Rat timeUnitSize, Int loadPeriod, Int resourceCycle, Int costGranularity) implements Master{
  NodeScheduler scheduler;
  MasterLoadbalancer mlb;

  {
    mlb = new MasterLoadbalancerObject();
    scheduler = new NodeSchedulerObject(cp,mlb,loadPeriod,resourceCycle);
  }

  Bool deployService(Service s){
    await s!deploy(scheduler);
    return True;
  }

  List<NodeState> getNodesStates(){
    List<NodeState> list = await scheduler!getNodeStates();
    return list;
  }

  List<Pair<String,ConsumptionMap>> getNodesMap(){
    List<Pair<String,ConsumptionMap>> result = list[];
    List<Node> nList = await scheduler!getNodes();

    foreach (n in nList){
      NodeResourcesMonitor nMonitor = await n!getMonitor();
      String nodeName = await n!getName();
      ConsumptionMap map = await nMonitor!getConsumptionMap();

      result = appendright(result,Pair(nodeName,map));
    }

    return result;
  }

  Bool createNodes(Int nNodes, Rat cpu, Rat memory, Rat baseSystemLoad, Map<String,Map<Int,Map<String,Rat>>> calibrationMaps){
    await this.scheduler!createNodes(nNodes, calibrationMaps, cpu, memory, timeUnitSize, baseSystemLoad, costGranularity);
    return True;
  }

  Unit setSchedulerRulesMap(Map<String,List<Int>> rulesMap){
    await this.scheduler!setRulesMap(rulesMap);
  }

  MasterLoadbalancer getEndpoint(){
    return this.mlb;
  }
}

//
// NODE
//
interface Node{
  String getName();
  NodeResourcesMonitor getMonitor();
  //Use Node cpu, equivalent to [Cost: amount] skip;
  Unit consumeCpu(Rat amount,Pod p);

  Rat allocateMemory(Rat amount);
  Rat releaseMemory(Rat amount);

  Unit addPod(Pod p, ResourcesMonitor rm, String serviceName);
  Unit removePod(Pod p, ResourcesMonitor rm);

  List<Pod> getPods();

  Rat getRequestedCpu();
  Rat getAvailableCpu();

  Unit processRequest(NodeRequest request);
}
[Scheduler: node_reset_resources_scheduler(queue)] class NodeObject(Int id, Rat cpu, Rat memory, Rat timeUnitSize, Int cycle,
                                                                    Rat baseSystemCpuLoad, NodeResourcesMonitor monitor,
                                                                    Int costGranularity,
                                                                    MasterLoadbalancer mlb,
                                                                    Map<String,Map<Int,Map<String,Rat>>> calibrationMaps // <WF , <RPS , <SERVICE , millicores>>>
                                                                    ) implements Node{
  List<ResourcesMonitor> pMonitors = list[];
  List<Pod> pods = list[];
  String name = "";
  Bool refreshed = False;
  Bool memoryLock = False;
  Bool computing = False;
  Bool sysLoadInterleaving = False; // to interleave pods and system consumptions on the node

  Rat availableCpu = 0;
  Rat availableMemory = 0;
  Rat systemCpuLoad = 0;

  Rat requestedCpu = 0;

  Int remainingSysLoadSteps = 0;

  // fixed nodes calibration features
  List<NodeRequest> nodeReqsQueue = list[];
  Map<String,Int> podsMap = map();
  Map<String,List<Pod>> servPodsReferenceMap = map();

  {
    this.name = ("Node-" + toString(id));
    availableCpu = cpu;
    availableMemory = memory;
    println("TYPE:" + "node_creation" + " " + "TU:" + toString(now()) + " " + "NODE:" + this.name + " " + "CPU_CAPACITY:" + toString(cpu)
            + " " + "MEMORY_CAPACITY:" + toString(memory));
    systemCpuLoad = baseSystemCpuLoad;
    remainingSysLoadSteps = costGranularity;
    this!resetResources();
    this!consumeSystemLoad();
    this!consumeSystemCpu();
  }

  String getName(){
    return this.name;
  }

  List<Pod> getPods(){
    return this.pods;
  }

  NodeResourcesMonitor getMonitor(){
    return this.monitor;
  }

  // Sets the consumptions on the monitor and reset the availableCpu to the full value.
  // this method is the last method called by the scheduler every time unit.
  Unit resetResources(){

    monitor!setConsumedCpu(cpu - availableCpu);
    monitor!setConsumedMemory(memory - availableMemory);

    availableCpu = cpu;
    systemCpuLoad = baseSystemCpuLoad;
    remainingSysLoadSteps = costGranularity;

    await duration(cycle,cycle);

    this!resetResources();
  }

  // flush the list of the node request computing the total costs for its services, generates and sends requests
  // with millicores costs directly to the pods, rps are balanced as round robin among the pods.
  // this method is executed once every time unit, after all the incoming requests for the time unit are in the list
  Unit convertRps(){
    Map<String,Map<String,Int>> serviceWfRpsMap = map(); // map to store the total consumption 'Service' -> Map<'Workflow',Rps>
    Map<String,Int> totRpsMap = map();

    // RACCOGLIERE RPS TOTALI PER <SERVIZIO,WF>
    for (nodeReq in nodeReqsQueue){ // cycle through the node requests in the queue
      // decompose the NodeRequest
      String reqService = service(nodeReq);
      String reqWf = wfName(nodeReq);
      Int reqRps = rps(nodeReq);

      // retrieve the current total rps for that <Service,Workflow> and compute the new amount
      Map<String,Int> sMap = lookupDefault(serviceWfRpsMap,reqService,map());
      Int currentRps = lookupDefault(sMap,reqWf,0);
      currentRps = currentRps + reqRps;

      // update the service and gloabl maps
      sMap = put(sMap,reqWf,currentRps);
      serviceWfRpsMap = put(serviceWfRpsMap,reqService,sMap);
    }

    services = keys(serviceWfRpsMap); // services involved

    for (service in services){ // create a map with <service,total rps>
      Map<String,Int> sMap = lookup(serviceWfRpsMap,service);

      List<Int> rpsValues = values(sMap);
      Int totalRps = foldr(+)(rpsValues,0); // total rps for that service
      totRpsMap = put(totRpsMap,service,totalRps);
    }

    // from the wfs and total rps maps generates the consumptions in millicores
    Map<String,Rat> serviceConsumptionsMap = buildServWfConsumptions(serviceWfRpsMap,totRpsMap);

    for (service in services){ // generates and sends the requests for the pods of the service
      Int totServRps = lookup(sMap,service);
      Int nOfServPods = lookup(podsMap,service);
      Rat serviceCost = lookup(serviceConsumptionsMap,service);
      Rat costPerRps = serviceCost / totServRps;

      List<Pod> servicePods = lookup(this.servPodsReferenceMap,service); // list of pods of the service, used to send the reqs

      List<Int> rpsQuotas = generatePodsRequestsSizes(totServRps,nOfServPods);

      Int index = 0;
      while (index < nOfServPods){ // cycle thorugh the list of quotas and the list of pods simoultaneously
        rpsQuota = nth(rpsQuotas,index); // <index> quota in the list
        Rat requestCost = costPerRps * rpsQuota; // compute the cost

        Request req = Request(service,requestCost,0,rpsQuota); // generate the request

        Pod p = nth(servicePods); // <index> pod reference in the list

        p!processRequest(req); // send the request

        index = index + 1;
      }
    }
  }

  // takes total-rps and per-workflow-rps maps and generates the millicores consumptions map using the conversion table
  Map<String,Rat> buildServWfConsumptions(Map<String,Map<String,Int>> serviceWfRpsMap, Map<String,Int> totRpsmap){
    Map<String,Rat> result = map();

    List<String> services = keys(totRpsMap); // all the services that received requests
    for (service in services){
      Int tRps = lookup(totRpsMap,service); // total rps for the service
      Map<String,Int> servWfRpsMap = lookup(serviceWfRpsMap,service); // map with (WF,RPS) for that service
      List<String> workflows = keys(servWfRpsMap); // Wfs involved for that service

      Map<String,Pair<Pair<Int,Rat>,Pair<Int,Rat>>> wfCalibEntries = buildWfEntries(tRps,workflows); // return the weighted entries in the conversionTable for the involved workflows

      Rat millicoresLoad = computeServiceLoad(service,wfCalibEntries,servWfRpsMap);
      result = put(result,service,millicoresLoad);
    }

    return result;
  }

  // takes a rps value and a list of involved workflows, and returns a map that associate to every workflow
  // the two nearest calibration entries (e1 < rps and e2 > rps) with their proportional weight,
  // these weights are computed from the differences between e1,e2 and rps.
  // example: rps=75, wf_entries=[50,100,150,200,250] => e1=50,weight=0.5 and e2=100,weight=0.5
  Map<String,Pair<Pair<Int,Rat>,Pair<Int,Rat>>> buildWfEntries(Int tRps, List<String> workflows){
    Map<String,Pair<Pair<Int,Rat>,Pair<Int,Rat>>> result = map();

    for (wf in workflows){
      Map<Int,Map<String,Rat>> wfEntMap = lookup(calibrationMaps,wf);
      List<Int> entries = keys(wfEntMap);

      Pair<Pair<Int,Rat>,Pair<Int,Rat>> weightedEntries = findNearestValues(tRps,entries);

      result = put(result,wf,weightedEntries);
    }

    return result;
  }

  // takes an Int N and a List<Int> and returns the nearest Ints (X1 and X2) in the list,
  // s.t. X1 < N and X2 > N, and computes their weights according to their distance from N.
  // If one of the elements of the list is equal to N: returns X1 with weight = 1 and the other one with weight = 0.
  //
  Pair<Pair<Int,Rat>,Pair<Int,Rat>> findNearestValues(Int tRps, List<Int> entries){
    Int higher = 10000;
    Int lower = -1;

    for (entry in entries){
      if (entry == tRps){ // exact entry value
        return Pair(Pair(tRps,1),Pair(0,0));
      } else if (entry < tRps && entry > lower){ // update lower value
        lower = entry;
      } else if (entry > tRps && entry < higher){ // update higher value
        higher = entry;
      }
    }

    // check if it's outside the calibration entries boundaries
    if (higher == 10000){
      return Pair(Pair(lower,1),Pair(0,0));
    } else if (lower == -1){
      return Pair(Pair(higher,1),Pair(0,0));
    }

    // compute weights
    Int distance = higher - lower; // distance between the two entries
    Rat lWeight = (higher - tRps) / distance; // lower weight: proportion of 'higher' from tRps over the total entries distance
    Rat hWeight = 1 - lWeight; // higher weight

    return Pair(Pair(lower,lWeight),Pair(higher,hWeight));
  }

  // takes a service and computes its total load in millicores. This is made through the rps values per workflow involved and
  // which entries to look for in the calibrationTable.
  Rat computeServiceLoad(String service, Map<String,Pair<Pair<Int,Rat>,Pair<Int,Rat>>> wfCalibEntries, Map<String,Int> servConsMap){
    Map<String,Rat> wfRawLoads = map[];
    List<String> wfs = keys(servConsMap);

    // retrieve and compute the wf values using the entries in the calibrationTable
    for (wf in wfs){
      // take the corresponding pair of entries and decompose them
      Pair<Pair<Int,Rat>,Pair<Int,Rat>> cEntries = lookup(wfCalibEntries,wf);
      Int e1 = fst(fst(cEntries)); Rat w1 = snd(fst(cEntries));
      Int e2 = fst(snd(cEntries)); Rat w2 = snd(snd(cEntries));

      // read from the calibration map of the node
      Map<Int,Map<String,Rat>> wfCalibMap = lookup(calibrationMaps,wf);
      Map<String,Rat> firstRpsEntryMap = lookup(wfCalibMap,e1); // services map for that entry
      Rat e1Millicores = lookup(firstRpsEntryMap,service); // read the millicores consumption for the service

      if (w1 < 1){ // there are actually two valid weighted entries
        Map<String,Rat> secondRpsEntryMap = lookup(wfCalibMap,e2);
        Rat e2Millicores = lookup(secondRpsEntryMap,service);

        Rat wfRawLoad = e1Millicores * w1 + e2Millicores * w2; // multiply the values for their weights
        wfRawLoads = put(wfRawLoads,wf,wfRawLoad); // update the resulting map
      } else { // bad entry pair, just one value
        wfRawLoads = put(wfRawLoads,wf,e1Millicores); // update the resulting map
      }
    }

    // weight these values according to the composition of the total rps
    Rat result = 0;
    Int totalRps = 0;
    for (wf in wfs){
      Rat rawLoad = lookup(wfRawLoads,wf);
      Int wfRps = lookup(servConsMap,wf);
      totalRps = totalRps + wfRps; // compute total rps
      result = result + rawLoad * wfRps; // add weighted load for the wf
    }

    result = result / totalRps; // divide for the total rps to get the actual milllicores value
    return result;
  }

  // returns a list wiht the quotas of rps for each pod given a number of rps and a number of pods.
  List<Int> generatePodsRequestsSizes(Int totRps, Int nOfPods){
    List<Int> result = list();

    Int rPerPod = truncate(totRps / nOfPods);
    Int remainingSize = totRps % nOfPods;

    Int index = 0;
    while (index < nOfPods){
      if (index < remainingSize){
        result = appendright(result,rPerPod + 1);
      } else {
        result = appendright(result,rPerPod);
      }
      index = index + 1;
    }

    return result;
  }

  // consumes remaining system load, if there, after all pod consumptions in the time unit
  Unit consumeSystemLoad(){
    await duration(cycle/2,cycle/2);

    availableCpu = availableCpu - systemCpuLoad;
    //println("{TIME " + toString(timeValue(now())) + "} " + "[NODE " + toString(id) + "]" + "SYSTEM consumed: " + toString(systemCpuLoad) + " SYSTEM DISCHARGE");
    systemCpuLoad = 0;

    await duration(cycle/2,cycle/2);

    this!consumeSystemLoad();
  }

  // consumes part of the system load
  Unit consumeSystemCpu(){
    await sysLoadInterleaving;
    await (remainingSysLoadSteps > 0);

    Rat toConsume = systemCpuLoad / remainingSysLoadSteps;

    availableCpu = availableCpu - toConsume;
    systemCpuLoad = systemCpuLoad - toConsume;

    remainingSysLoadSteps = remainingSysLoadSteps - 1;
    sysLoadInterleaving = False;

    this!consumeSystemCpu();
  }

  Unit consumeCpu(Rat amount,Pod p){
    Bool done = False;
    Rat servAvailCpu = 0;

    while (!done){
      await (availableCpu - systemCpuLoad) > 0;
      servAvailCpu = availableCpu - systemCpuLoad;

      if (servAvailCpu >= amount){
        availableCpu = availableCpu - amount;

        sysLoadInterleaving = True;
        done = True;
      } else {
        amount = amount - servAvailCpu;
        availableCpu = availableCpu - servAvailCpu;
        p!setBlocked();
        await (availableCpu - systemCpuLoad) > 0;
      }
    }
  }

  Rat allocateMemory(Rat amount){
    Rat givenMemory = 0;
    await !this.memoryLock;
    this.memoryLock = True;

    if (availableMemory >= amount){
      availableMemory = availableMemory - amount;
      givenMemory = amount;
    }

    this.memoryLock = False;

    return givenMemory;

  }

  Rat releaseMemory(Rat amount){
    Rat releasedMemory = 0;
    await !this.memoryLock;

    this.memoryLock = True;
    availableMemory = availableMemory + amount;
    this.memoryLock = False;

    releasedMemory = amount;

    return releasedMemory;
  }

  Unit addPod(Pod p, ResourcesMonitor rm, String serviceName){
    Rat reqCpu = await rm!getCpuRequest();
    this.pMonitors = appendright(this.pMonitors,rm);
    this.pods = appendright(this.pods,p);

    this.requestedCpu = this.requestedCpu + reqCpu;
    monitor!setRequestedCpu(this.requestedCpu);

    // update the podsMaps
    List<Pod> servPods = lookupDefault(this.servPodsReferenceMap,serviceName,list[]);
    servPods = appendright(servPods,p);
    this.servPodsReferenceMap = put(this.servPodsReferenceMap,servPods);

    Int nPods = lookupDefault(podsMap,serviceName,-1);
    if (nPods == -1){
      this.podsMap = put(this.podsMap,serviceName,1);
    } else {
      this.podsMap = put(this.podsMap,serviceName,nPods + 1);
    }
    await mlb!updateNodeMap(this,this.podsMap);
  }

  Unit removePod(Pod p, ResourcesMonitor rm, String serviceName){
    Rat reqCpu = await rm!getCpuRequest();
    this.pMonitors = without(this.pMonitors,rm);
    this.pods = without(this.pods,p);

    this.requestedCpu = this.requestedCpu - reqCpu;
    monitor!setRequestedCpu(this.requestedCpu);

    // update the podsMap
    List<Pod> servPods = lookupDefault(this.servPodsReferenceMap,serviceName,list[]);
    servPods = without(servPods,p);
    this.servPodsReferenceMap = put(this.servPodsReferenceMap,servPods);

    nPods = lookupDefault(podsMap,serviceName,-1);
    if (nPods == -1 || nPods == 0){
      println("===== ===== ERROR: REMOVING A POD FROM AN EMPTY NODE ===== =====");
      throw Exception;
    } else {
      this.podsMap = put(this.podsMap,serviceName,nPods - 1);
    }
    await mlb!updateNodeMap(this,this.podsMap);
  }

  List<ResourcesMonitor> getPMonitors(){
    return this.pMonitors;
  }

  Rat getRequestedCpu(){
    return this.requestedCpu;
  }

  Rat getAvailableCpu(){
    return this.cpu - this.requestedCpu;
  }

  Unit processRequest(NodeRequest request){
    this.nodeReqsQueue =  appendright(this.nodeReqsQueue,request);
  }

}

//
// NODE BALANCER
//
interface NodeScheduler{
  //create nNodes with given cpu and memory amounts.
  Bool createNodes(Int nNodes, Rat cpu, Rat memory, Rat timeUnitSize, Rat baseSysConsumption, Int costGranularity);

  List<NodeState> getNodeStates();
  List<Node> getNodes();

  Node deployPod(Pod p,ResourcesMonitor rm);

  Unit setRulesMap(Map<String,List<Int>> rulesMap);
}

//loadPeriod: period to consider to select the less loaded node
class NodeSchedulerObject(CloudProvider cloud, MasterLoadbalancer mlb, Int loadPeriod, Int resourceCycle) implements NodeScheduler{
  // map of id:node
  Map<Int,Node> nodesMap = map[];
  Map<String,List<Int>> rulesMap = map[];
  // static list of all the nodes
  List<Node> nodes = list[];
  // list used to schedule new pods, can change order
  List<Node> activeNodes = list[];
  Int nextID = 1;

  // pod deploy lock
  Bool deploying = False;

  Bool createNodes(Int nNodes, Map<String,Map<Int,Map<String,Rat>>> calibrationMaps,
                   Rat cpu, Rat memory, Rat timeUnitSize, Rat baseSysConsumption, Int costGranularity){
    Int ctr = 0;
    while (ctr < nNodes) {
      NodeResourcesMonitor monitor = new NodeResourcesMonitorObject(resourceCycle,cpu,memory);
      Node node = new NodeObject(nextID, cpu, memory, timeUnitSize, resourceCycle,
                                 baseSysConsumption, monitor, costGranularity,
                                 mlb, calibrationMaps);
      await monitor!setNode(node);
      nodes = appendright(nodes,node);
      activeNodes = appendright(activeNodes,node);
      nodesMap = insert(nodesMap,Pair(nextID,node));

      this.nextID = nextID + 1;
      ctr = ctr + 1;
    }
    println("[Time: "+toString(timeValue(now()))+"] *********** CREATED "+toString(nNodes)+" NODES [CPU: " + toString(cpu) + " - " + "MEMORY: " + toString(memory) + " ]");
    return True;
  }

  //returns the node with more not requested cpu available
  Node deployPod(Pod p, ResourcesMonitor rm){
    Bool deployed = False;
    Rat requestedCpu = await rm!getCpuRequest();
    String serviceName = await rm!getServiceName();
    Node result = null;
    List<Node> nodesToCheck = list[];

    List<Int> nodeIds = lookupDefault(rulesMap,serviceName,list[]);

    if (!isEmpty(nodeIds)){
      Node selected = lookupDefault(nodesMap,head(nodeIds),null);
      if (selected != null){
        nodesToCheck = list[selected];
        rulesMap = put(rulesMap,serviceName,cycleIntList(nodeIds)); // put the first node at the bottom of the list of the rule
      }
    }

    if (isEmpty(nodesToCheck)){ // standard behavior
      await (!isEmpty(activeNodes));
      nodesToCheck = activeNodes;
    }

    while (!deployed){
      await !deploying;
      this.deploying = True;
      Rat maxCpu = -1;

      foreach ( n in nodesToCheck){
        Rat cpu = await n!getAvailableCpu();
        if (cpu > maxCpu){
          maxCpu = cpu;
          result = n;
        }
      }

      if (maxCpu >= requestedCpu){
        await result!addPod(p,rm,serviceName);
        this.deploying = False;
        deployed = True;
      }else{
        this.deploying = False;
        await duration(1,1);
      }
    }
    return result;
  }

  List<NodeState> getNodeStates(){
    List<NodeState> list = list[];

    foreach (node in nodes){
      NodeResourcesMonitor monitor = await node!getMonitor();
      NodeState ns = await monitor!getNodeState();
      list = appendright(list,ns);
    }

    return list;
  }

  List<Node> getNodes(){
    return this.nodes;
  }

  Unit setRulesMap(Map<String,List<Int>> rulesMap){
    this.rulesMap = rulesMap;
  }
}

interface MasterLoadbalancer(){
  Unit sendClientRequest(ClientRequest request);

  Unit addNode(Node n, Map<String,Int> podsMap);

  Unit updateNodeMap(Node node, Map<String,Int> podsMap);
}
class MasterLoadbalancerObject() implements MasterLoadbalancer{
  List<Node> activeNodes = list[]; // currently active nodes in the system
  Map<Node,Map<String,Int>> activeNodesMaps = map(); // map contating the maps that indicates how many pods of a chosen service are deployed on the node
                                             // i.e. node1 -> [activeNodesMaps] = node1PodsMap; "service1" -> [node1PodsMap] = number of pods of service1 deployed on node1
  Map<String,Map<Node,Int>> perServicePodsNumber = map(); // service based map, better for balancing

  Unit sendClientRequest(ClientRequest request){
    this.balanceClientRequest(request);
  }

  Unit balanceClientRequest(ClientRequest request){
      WorkflowData wfd = wf(request);
      List<String> services = services(wfd);

      foreach (service in services){ // cycle through services of the workflow and balance the rps with a round-robin policy among the pods
        Map<Node,Int> servicePodMap = lookup(perServicePodsNumber,service);
        List<Pair<Node,Int>> quotas = generateSubrequestsSizes(rps(request),servicePodMap);

        foreach (quota in quotas){ // generate and forward the sub-requests
          Node node = fst(quota);
          Int size = snd(quota);

          NodeRequest nodeReq = NodeRequest(service,wfName(wfd),size); // create the node request (service,wf,rps) and sends it to the node
          node!processRequest(nodeReq); // send the request to the node
        }
      }
  }

  // takes a size and a podMap of a service and computes the number of request for every node, the policy is round-robin
  // between the pods.
  // the remaining K requests (K < Total number of pods for the service) are just given to the first K pods. (not balancing between the nodes)
  List<Pair<Node,Int>> generateSubrequestsSizes(Int rSize, Map<Node,Int> podsMap){
    List<Pair<Node,Int>> result = list();
    Int totalPods = foldr(+)(values(podsMap), 0);

    rPerPod = truncate(rSize / totalPods);
    remainingSize = rSize % totalPods;

    foreach (node in keys(podsMap)){
      Int nodePods = lookup(podsMap,node);
      Int subReqSize = rPerPod * nodePods;

      if (remainingSize >= nodePods){
        remainingSize = remainingSize - nodePods;
        subReqSize = subReqSize + nodePods;
      } else if (remainingSize > 0){
        subReqSize = subReqSize + remainingSize;
        remainingSize = 0;
      }

      result = appendright(result,pair(node,subReqSize));
    }

    return result;
  }

  Unit updateNodeMap(Node node, Map<String,Int> podsMap){
    this.activeNodesMaps = put(this.activeNodesMaps,node,podsMap);
    this.updatePerServicePods(node,podsMap);
  }

  // updates the number of pods in the service based map
  Unit updatePerServicePods(Node node, Map<String,Int> map){
    List<String> services = list(keys(map));

    foreach (service in services){ // for every service of the updated node:
      podMap = lookupDefault(perServicePodsNumber,service,map()); // take the map of that service
      podsNumber = lookup(map,service); // read the updated number of pods for that node
      podMap = put(podMap,node,podsNumber); // udpate the node entry in the service map

      perServicePodsNumber = put(perServicePodsNumber,service,podMap); // update the service entry in the perService map
    }
  }
}
